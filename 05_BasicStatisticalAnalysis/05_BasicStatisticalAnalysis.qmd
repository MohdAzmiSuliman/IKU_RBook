# Statistical Tests

Several statistical tests will be covered, which will be grouped into

-   Bivariate analyses of categorical data
-   Bivariate analyses of numerical data
-   Linear regression
-   Logistic regression

This book however was not intended to be a statistical book, nor should not be the main reference for statistical analyses. Please refer your statistical texts book for further information.

In addition of conducting the test in standard way, I'll also how to create a nice result table using related packages.

::: callout-note
dataset used in this example can be downloaded from github site: https://github.com/MohdAzmiSuliman/IKU_RBook/tree/master/dataset
:::

## Practical: Setup Project

1.  Setup your project
    -   Open your RStudio
    -   Create New Project
2.  Create Quarto document
    -   update the YAML metadata to make the document self-contained

```{r}
#| echo: true
#| eval: false

---
title: "Sesi 3 - Basic Statistical Analysis"
format:
  html:
    embed-resources: true
---

```

## Bivariate Analyses of Categorical Data

Bivariate analysis involves examining the relationship or association between to variables. When both variables are categorical, bivariate analysis looks at how these categories are related. The bivariate analyses often involve the use of contingency tables, also know as cross-tabulation or crosstab.

### Contingency Table

Contigency table, or crosstab, is a simple matrix that display the frequency of occurence of combination of two categories for two categorical variable.

For example, this contingency table show a cross tabulation between smoking status and lung cancer outcome.

```{r}
#| echo: false
#| tbl-cap: "Contigency Table Structure"

library(tibble)
library(gt)

tribble(
  ~Status,        ~"Lung Cancer", ~"No Lung Cancer", ~"Total",
  "Smoker",       "a",      "b",         "a+b",
  "Non-Smoker",   "c",      "d",         "c+d",
  "Total",        "a+c",    "b+d",       "N"
) %>% 
  gt() %>% 
  cols_align(align = "center", columns = everything())
```

In this example:

-   the independent variable (or the predictor, i.e. the smoking status), is the row
-   the dependent variable (or the outcome, i.e. the cancer status), is the column
-   The letters a, b, c, and d represent the cell frequencies:
    -   a: The number of smokers who have the disease.
    -   b: The number of smokers who do not have the disease.
    -   c: The number of non-smokers who have the disease.
    -   d: The number of non-smokers who do not have the disease.
-   "Total" is the sum of the frequencies in the respective row or column, with N being the grand total of all observations.

\newpage

Example: Gender vs Employment Status

0.  Download dataset
    -   we will be using the `asthmads_clean.sav` dataset
    -   this code below download directly from github package into your working directory
    -   otherwise, you may download from the link: https://github.com/MohdAzmiSuliman/IKU_RBook/ raw/master/dataset/asthmads_clean.sav and copy to the working directory

```{r}
#| echo: fenced
#| eval: false

download.file(
  url = "https://github.com/MohdAzmiSuliman/IKU_RBook/raw/master/dataset/asthmads_clean.sav", 
  destfile = "asthmads_clean.sav", mode = "wb")
```

1.  Import dataset

```{r}
#| echo: fenced
library(tidyverse)
library(haven)

asthmads <- read_sav("asthmads_clean.sav") %>% 
  as_factor()
asthmads
```

2.  Create Contingency Table

```{r}
#| echo: fenced

with(asthmads, table(Gender, WorkStatus))
```

3.  ***BONUS 1***: We can also use `tidyverse::`, `janitor::` and `gt::` package to create nice looking table with total row and total column

```{r}
#| echo: fenced
#| tbl-cap: "2x2 Contigency Table for Gender by Employment Status, using tidyverse, janitor and gt"

asthmads %>%
  count(Gender, WorkStatus) %>%
  pivot_wider(names_from = WorkStatus, values_from = n, 
              values_fill = list(n = 0)) %>%
  janitor::adorn_totals(c("row", "col")) %>% 
  gt()
```

3.  ***BONUS 2***: We can also use `gtsummary::` package to create nice looking table with total row and total column

```{r}
#| echo: fenced
#| tbl-cap: "2x2 Contigency Table for Gender by Employment Status, using gtsummary"

library(gtsummary)

asthmads %>%
  tbl_summary(include = Gender, 
              by = WorkStatus, 
              digits = all_categorical() ~ c(0,1))
```

### Pearson's Chi-Square Test

Pearson Chi-square Test is a statistical test used to determine whether there is a significant association between two categorical variable.

Pearson Chi-square Test is conducted with these assumptions:

-   both variables must be **categorical** (either nominal or ordinal)
-   the observation must be **independence**. This mean that the outcome of one observation is not influency by the outcome of another observation
-   the groupings are **mutually exclusive**
-   \<20% of celss have **expected frequency of \<5**
-   minimum **expected frequency if \>1**

Example: Gender vs Employment Status

1.  Refer previous part to create contingency table
2.  Calculate the chi-square test

```{r}
#| echo: fenced

with(asthmads, table(Gender, WorkStatus)) %>% 
  chisq.test(., correct = F)
```

3.  Check the test's assumptions

```{r}
#| echo: fenced

with(asthmads, table(Gender, WorkStatus)) %>% 
  chisq.test(., correct = F) %>% 
  .$expected
```

::: callout-tip
If the assumptions for chi-square is violated, R will show warning message

**`In chisq.test(.) : Chi-squared approximation may be incorrect`**

which is most likely due to violation of small sample assumptions.
:::

4.  ***Bonus***: we can create a nice looking table with gtsummary

```{r}
#| echo: fenced
#| tbl-cap: "Association between Gender and Employment Status"

library(gtsummary)

asthmads %>%
  tbl_summary(include = Gender, 
              by = WorkStatus, 
              digits = all_categorical() ~ c(0,1)) %>% 
  add_p(test = all_categorical() ~ "chisq.test", 
        test.args = all_tests("chisq.test") ~ list(correct = F))
```

### Small Sample

Pearson's Chi-square is only reliable with medium to large datasets. There are two (2) assumptions in the test that might be violated for a small sample, which were (1) \<20% of cells have an expected frequency of \>5 and (2) minimum expected frequency \> 1.

There are two alternatives for a small sample, namely:

-   Yates' Correction for Continuity
-   Fisher Exact Test

#### Yates' Correction for Continuity

Yates' correction adjust the Chi-square statistics to account for the overestimation of significance due to the continuity assumption of the chi-square distribution.

In R, Yates' correction is the default option for Pearson Chi-square

::: callout-note
While Yates' correction is used for small sample (especially when it violate the chi-square assumptions), Yates' correction is not preferable for moderate or large sample, as it can be overly conservative, increasing the risk of Type II error
:::

```{r}
#| echo: fenced

with(asthmads, table(Gender, WorkStatus)) %>% 
  chisq.test(.)
```

```{r}
#| echo: fenced

asthmads %>%
  tbl_summary(include = Gender, 
              by = WorkStatus, 
              digits = all_categorical() ~ c(0,1)) %>% 
  add_p()
```

#### Fisher's Exact Test

Pearson's Chi-square test (and the Yates' Correction) is an approximation test, while Fisher Exact Test was based on calculating the exact probability of observing the data under the null hypothesis. Thus it is preferable compared to Pearson's Chi-square Test especially in small sample.

\newpage

```{r}
#| echo: fenced

with(asthmads, table(Gender, WorkStatus)) %>% 
  fisher.test(.)
```

```{r}
#| echo: fenced

asthmads %>%
  tbl_summary(include = Gender, 
              by = WorkStatus, 
              digits = all_categorical() ~ c(0,1)) %>% 
  add_p(test = all_categorical() ~ "fisher.test")
```

### McNemar Test

McNemar test is Pearson's Chi-square Test equivalence for paired data (e.g., pre-post data).

Example: Wheezing Symptom Pre and Post Intervention

1.  Create Contingency Table

```{r}
#| echo: fenced

with(asthmads, table(SxWheeze_Pre, SxWheeze_Post))
```

2.  Calcualte the McNemar Test statistic

```{r}
#| echo: fenced

with(asthmads, table(SxWheeze_Pre, SxWheeze_Post)) %>% 
  mcnemar.test(correct = F)
```

::: callout-caution
Supposedly `gtsummary::` package can create table for paired data, unfortunately I unable to replicate the code, despite converted to long format.
:::

## Bivariate Analyses of Numerical Data

Bivariate analysis involves examining the relationship or association between two variables, and bivariate analyses of numerical data is said when involving continuous dependent variables. Example of bivariate analyses of numerical data include

-   Parametric Tests
    -   Independent t-test
    -   Paired t-test
    -   Analysis of Variance (ANOVA)
    -   Pearson' Correlation
-   Non-Parametric Tests
    -   Mann-Whitney U Test
    -   Wilcoxon Signed Rank Test
    -   Spearman's Correlation

Many of parametric tests were based on normal distribution and analyses of the variances, while non-parametric tests were based on rank.

### Independent T-test

Independent t-test is a parametric test, commonly used to compare the mean of two independent sample, more specifically, compare the difference of the two means, in relation to the variation (i.e., the variance) of the data.

\newpage

Independent t-test is conducted with these assumptions:

-   samples was taken **randomly** (i.e., the sample representative of the population)
-   the groups and measurements were **independents**.
-   the outcome (or dependent variable) is **numerical** data.
-   the outcome is **normally distributed** in each group.
-   *the variance outcome between groups is approximately equal (**homogeneity of variance**)*

::: callout-note
1.  In large sample size, T-test can be robust to violation of normal distribution assumption, based on Central Limit Theorem.
2.  The homogeneity of variance can be ignore if we use Welch T-test as default.
:::

Example: Compare Height between Gender

1.  We will use the same dataset as previous `asthmads`.
    -   below is the code as recap

```{r}
#| echo: fenced
#| eval: false

download.file(
  url = "https://github.com/MohdAzmiSuliman/IKU_RBook/raw/master/dataset/asthmads_clean.sav", 
  destfile = "asthmads_clean.sav", mode = "wb")

asthmads <- read_sav("asthmads_clean.sav") %>% 
  as_factor()
```

```{r}
#| echo: fenced

asthmads
```

2.  Confirm data distribution
    -   assumption for normal distributed in each group

```{r}
#| echo: fenced

asthmads %>% 
  ggplot(aes(x = Ht_m, fill = Gender)) +
  geom_density(alpha = .5) +
  theme_bw()
```

3.  Calculate mean and SD of height for each gender

```{r}
#| echo: fenced

asthmads %>% 
  group_by(Gender) %>% 
  summarise(mean = mean(Ht_m, na.rm = T),
            sd = sd(Ht_m, na.rm = T))
```

4.  Conduct the Welch's T-test

```{r}
#| echo: fenced

t.test(Ht_m ~ Gender, asthmads)
```

::: callout-important
In R, many functions require a formula parameter, especially in statistical modelling.

-   The general form of a formula is `outcome ~ predictors, data`,
    -   outcome = dependent variable
    -   predictors = independent variable**s**.
-   This formula structure is used in various functions, such as linear modelling.
-   For a t-test, which compares means across groups,
    -   the formula formed by `outcome ~ group, data`
    -   `group` = categorical variable = groups
:::

5.  ***Bonus 1***: We can create a nice looking table with gtsummary

```{r}
#| echo: fenced
#| tbl-cap: "Height Differences between Gender table, using gtsummary"

asthmads %>% 
  tbl_summary(include = Ht_m, 
              by = Gender, 
              statistic = all_continuous() ~ "{mean} ({sd})") %>% 
  add_difference()
```

6.  ***Bonus 2***: the *classical* Student T-test

As mention previously, by default, R will use Welch's T-test, regardless of the assumption of homogeneity of variance. But if you still want to use the Student T-test, change the `var.equal = TRUE` parameter

```{r}
#| echo: fenced

t.test(Ht_m ~ Gender, asthmads, var.equal = T)
```

::: callout-important
Interested to know why Welch's T-test is preferable? In short:

-   Student T-test result biased when assumption for normality and homogeneity of variance were not met
-   Welch's T-test provides better control of Type 1 error rates when assumption of homoegeneity of variance was not met.
-   Using Welch's T-test as default skip the need to test for homogeneity of variance (i.e., the Levene's test)
-   Real data commonly not normally distributed and it is reasonable to assume that variance is unequal in many studies

Source: <https://doi.org/10.5334/irsp.82>
:::

### Paired T-test

Paired T-test is a parametric test, used to compare the mean of two dependent sample, e.g., comparing the mean for pre and post measurement.

Independent t-test is conducted with these assumptions:

-   The samples were **random**
-   The measurement were **dependent** (i.e., paired, e.g., pre-post)
-   The outcome variable is **numerical data**
-   The **outcome differences** is **normally distributed**

Example: Weight differences from pre to post measurement

1.  We will use the same dataset as previous: `asthmads`

2.  Check the differences data distribution

    -   assumption for outcome difference is normally distributed
    -   we need to calculate the differences

    ```{r}
    #| echo: fenced
    #| eval: false

    asthmads %>% 
      mutate(Weight_Diff = Weight_Post - Weight_Pre)
    ```

    -   then plot the differences to check the distribution

    ```{r}
    #| echo: fenced

    asthmads %>% 
      mutate(Weight_Diff = Weight_Post - Weight_Pre) %>% 
      ggplot(aes(x = Weight_Diff)) + 
      geom_density(alpha = .5, fill = "blue") + 
      theme_bw()
    ```

    -   we can see that the data is approximately normally distributed

3.  Calculate mean and SD of weight - pre & post

```{r}
#| echo: fenced

asthmads %>%
  summarise(Weight_Pre_Mean = mean(Weight_Pre, na.rm = T), 
            Weight_Pre_SD = sd(Weight_Pre, na.rm = T), 
            Weight_Post_Mean = mean(Weight_Post, na.rm = T), 
            Weight_Post_SD = sd(Weight_Post, na.rm = T))
```

4.  Conduct the Paired T-test

```{r}
#| echo: fenced

t.test(Pair(Weight_Post, Weight_Pre) ~ 1, asthmads)
```

::: callout-caution
Since this is pre-post test, we need to write the post column first. In this example, the samples had reduction in body weight, thus the differences is in negative
:::

5.  ***Bonus***: we can create a nice looking table with gtsummary

```{r}
#| echo: fenced

asthmads %>%
  select(id, Weight_Post, Weight_Pre) %>%
  pivot_longer(cols = starts_with("Weight"),
               names_to = "Type",
               values_to = "Weight",
               names_pattern = "Weight_(.*)") %>%
  mutate(Type = fct_relevel(Type, "Pre")) %>%
  tbl_summary(by = Type,
              include = Weight,
              statistic = all_continuous() ~ "{mean} ({sd})",
              digits = all_continuous() ~ 2) %>% 
  add_difference(test = all_continuous() ~ "paired.t.test", 
                 group = id)
  
```

### ANOVA - Analysis of Variance

Analysis of Variance (ANOVA) is a parametric test, commonly used to compare the means of three or more independent (unrelated) groups, more specifically, in relation to the variation (i.e. the variance) of the data.

::: callout-note
There are severals test in ANOVA, which include one-way ANOVA, two-way ANOVA, N-way ANOVA, ANCOVA, Repeated Measure ANOVA, RM ANCOVA, MANOVA etc.

In this example, we will focus only to one-way ANOVA
:::

One-way ANOVA is conducted with these assumptions:

-   The samples were taken **randomly**
-   The groups and measurements were **independents**
-   The outcome is **numerical** data
-   The outcome in approximately **normal distribution** in each group
-   The variances of outcome in each group have approximately equal, i.e. **homogeneity of variance**.

::: callout-tip
note that almost all parametric tests have similar assumptions.
:::

Example: PEFR improvement between Treatment Group

1.  We will use the same dataset as previous, i.e., the `asthmads`
    -   we will need to wrangle the dataset, to calculate the PEFR improvement

```{r}
#| echo: fenced

asthmads <- asthmads %>% 
  mutate(PEFR_Diff = PEFR_Post-PEFR_Pre)
  
```

2.  Confirm data distribution
    -   assumption for normal distributed in each group

```{r}
#| echo: fenced

asthmads %>% 
  ggplot(aes(x = PEFR_Diff, fill = Tx2)) + 
  geom_density(alpha = .5) + 
  theme_bw()
```

3.  Calculate mean and SD of PEFR_Diff for each treatment group

```{r}
#| echo: fenced

asthmads %>% 
  group_by(Tx2) %>% 
  summarise(mean = mean(PEFR_Diff, na.rm = T), 
            sd = sd(PEFR_Diff, na.rm = T))
```

4.  Conduct the ANOVA

```{r}
#| echo: fenced

oneway.test(PEFR_Diff ~ Tx2, asthmads)
```

::: callout-note
In R, there are three related functions, but used differently, for ANOVA.

1.  Function `oneway.test(_)` is for Welch's ANOVA
2.  Function `aov(_)` is for ANOVA assuming equal variance
3.  Function `anova(_)` is to compare between models.
:::

5.  Pair-wise post-hoc test
    -   since the comparison by ANOVA is significant, we need to do post-hoc test to check which pair have significant difference.
    -   since there are three pairwise comparison (Drug A vs B, Drug A vs Placebo and Drug B vs Placebo), in Bonferroni, the alpha level need to divide by 3 (i.e., 0.017)

::: callout-important
Type 1 error increase with the number of comparison being made. **Thus adjustment to the alpha level are necessary**.

Common post-hoc adjustment include Bonferroni Correction, in which the original alpha level (\alpha) divided by the number of comparison.
:::

```{r}
#| echo: fenced

pairwise.t.test(asthmads$PEFR_Diff, asthmads$Tx2, 
                p.adjust.method = "b")
```

we can see that only Drug A vs B is not significant. other pairs were significant.

6.  ***Bonus***: we can create nice looking table with gtsummary

```{r}
#| echo: fenced

asthmads %>% 
  tbl_summary(include = PEFR_Diff, 
              by = Tx2, 
              label = PEFR_Diff ~ "PEFR Difference (Post - Pre)", 
              statistic = all_continuous() ~ "{mean} ({sd})", 
              digits = all_continuous() ~ 2) %>% 
  add_p(test = all_continuous() ~ "oneway.test")
```

### Pearson's Correlation

Pearson's correlation is a parametric test, commonly used to test correlation between two numerical data.

The Pearson's Correlation assumptions were:

-   each observation/measurement were **independent**
-   both variables were **numerical** data
-   both variables should follow approximately **normal distribution**.
-   the relationship between the two variables should be **linear**.
-   the scatterplot of the two variables should show a roughly constant spread, the **homoscedasticity**

::: callout-tip
homogeneity and homoscedasticity is similar concept but mean differently. homogeneity means the variance between different groups were approximately equal, but homoscedasticity means the variances of residuals or best fit line were approximately constant.
:::

Example: Correlation between height and body weight

1.  We will use the same dataset as previous, i.e. the `asthmads`.

2.  Confirm data distribution

-   assumptions for normal distribution

```{r}
#| echo: fenced
#| layout-ncol: 2

asthmads %>% 
  ggplot(aes(x = Ht_m)) + 
  geom_density(fill = "lightblue", alpha = .5) + 
  theme_bw()

asthmads %>% 
  ggplot(aes(x = Weight_Pre)) + 
  geom_density(fill = "lightgreen", alpha = .5) + 
  theme_bw()
```

-   assumption for linearity & homoscedasciticity

```{r}
#| echo: fenced

modslope <- lm(Weight_Pre ~ Ht_m, data = asthmads) %>%
  broom::tidy() %>%
  filter(term == "Ht_m") %>%
  pull(estimate)

asthmads %>% 
  ggplot(aes(x = Ht_m, y = Weight_Pre)) + 
  geom_point(position = position_jitter()) + 
  geom_smooth(method = "lm", se = F) +
  geom_abline(slope = modslope, intercept = -70, 
              colour = "red", linetype = 2) +
  geom_abline(slope = modslope, intercept = -110, 
              colour = "red", linetype = 2) + 
  theme_bw()
```

4.  Calculate the pearson correlation

```{r}
cor.test(~ Ht_m + Weight_Pre, asthmads, method = "pearson")
```

5.  ***Bonus 1***: Create nice table with `apaTables::` package

```{r}
#| echo: fenced

library(apaTables)

asthmads %>% 
  select(Ht_m, Weight_Pre) %>% 
  apa.cor.table(.)
```

6.  ***Bonus 2***: Correlation Plot

```{r}
#| echo: fenced

library(corrplot)

asthmads %>% 
  select(-id) %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot(type = "upper", order = "hclust", 
           tl.col = "black", tl.srt = 45)
```

## Linear Regression

Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.

The formula for linear regression is

$$y = \beta_0 + \beta_1x + \epsilon$$

where

-   $y$ is the dependent variable (outcome)
-   $x$ is the independent variable (predictor)
-   $\beta_0$ is the y-intercept
-   $\beta_1$ is the slope of the line, representing the change in y for one-unit change in x
-   $\epsilon$ is the error term

For linear regression to provide reliable estimates, several key assumptions must be met

-   each observation/measurement were **independent**
-   the outcome variable is **numerical** data
-   the residuals follow approximately **normal distribution**.
-   the relationship between the predictor and the outcome should be **linear**.
-   the residuals have constants variance at every level of the predictor, i.e., the **homoscedasticity**
-   in multiple linear regression, there will be no or little **multicollinearity**, which independent variables are not too highly correlated with each other

::: callout-tip
note the similarity between pearson correlation and linear regression
:::

Example: Finding factors associated with BMI changes

#### Simple Linear Regression

1.  We will use the same dataset as previous, i.e. the `asthmads`
    -   we need to do some data wrangling to calculate the BMI changes

```{r}
#| echo: fenced

asthmads <- asthmads %>% 
  mutate(BMI_Diff = BMI_Post-BMI_Pre)
```

2.  Conduct the simple linear regression
    -   in this example, the simple linear regression model is saved in R object, for used later

```{r}
#| echo: fenced

slinm <- lm(BMI_Diff ~ PA_HW, asthmads)

summary(slinm)
```

3.  Test assumptions

-   the simplest way for assumptions testing, is to use `plot(_)` function

```{r}
#| layout-ncol: 2
#| echo: fenced

plot(slinm)
```

-   otherwise, we call also use `augment(_)` function from `broom::` package to extract the residuals and check the assumptions

```{r}
#| echo: fenced

library(broom)

augment(slinm)
```

-   using the dataset from augment, to plot the residual to check for normality

```{r}
#| echo: fenced

augment(slinm) %>% 
  ggplot(aes(.fitted)) +
  geom_density(fill = "lightblue", alpha = .5) +
  theme_bw()
```

-   plot residual values vs predicted values (aka fitted values) for linearity and homoscedasticity
    -   linearity: abscence of systematic patterns like curves
    -   homoscedasticity: residuals randomly scttered around the horizontal line with no funnel-shapped patern

```{r}
#| echo: fenced

augment(slinm) %>% 
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point(position = position_jitter()) + 
  geom_hline(yintercept = 0, linetype = 2, colour = "blue") +
  geom_hline(yintercept = .75, linetype = 3, colour = "red") +
  geom_hline(yintercept = -.75, linetype = 3, colour = "red") +
  theme_bw()
```

4.  Check model fitness

-   one of parameter to check our model fitness is by looking at the coefficient of determination $r^2$ from the model summary

```{r}
#| echo: fenced

summary(slinm)
```

-   we can also plot our observed vs predicted value to visualize the model fitness
    -   in this plot, the point should be close at the diagonal, i.e. observed = predicted

```{r}
#| echo: fenced

slinm %>% 
  augment() %>% 
  ggplot(aes(x = .fitted, y = BMI_Diff)) + 
  scale_x_continuous(limits = c(-6, 0)) +
  scale_y_continuous(limits = c(-6, 0)) +
  geom_point(position = position_jitter()) +
  geom_abline(intercept = 0, slope = 1, linetype = 2) + 
  theme_bw() +
  theme(aspect.ratio = 1)
```

5.  ***Bonus***: nice table with gtsummary

-   `tbl_uvregression(_)` functions from gtsummary not only can create a nice table

```{r}
#| echo: fenced

asthmads %>% 
  tbl_uvregression(method = lm,
                   y = BMI_Diff,
                   include = PA_HW)
```

-   but also can simplified our simple linear regression involving several predictors

```{r}
#| echo: fenced

asthmads %>% 
  select(BMI_Diff, Gender, Age, WorkStatus, Height, PA_HW) %>% 
  tbl_uvregression(method = lm,
                   y = BMI_Diff, 
                   pvalue_fun = partial(style_pvalue, 
                                        digits = 3)) %>% 
  bold_p()
```

### Multiple Linear Regression

Real-world relationships between variables rarely occur in isolation; they are often influenced by third variables. These third variables can be categorized as:

-   Confounders
-   Mediators
-   Moderators

Controlling for these third variables is crucial for accurately understanding the relationships among variables. While the optimal approach is to control for these variables at the study design level (e.g., through randomization, stratification, and setting proper inclusion and exclusion criteria), the presence of third variables can be inevitable and may significantly impact the observed relationships.

When third variables cannot be controlled through design alone, statistical methods can be employed to adjust for their influence, including (not limited to) Stratification, Multivariable Analysis or Multivariate Analysis

Example: Find factors associated with BMI changes

1. Conduct multiple linear regression
    -   and check for model fitness ($R^2$)

```{r}
#| echo: fenced

mlinm <- lm(BMI_Diff ~ Gender + Age + WorkStatus + PA_HW + BMI_PreCat, 
            asthmads)

summary(mlinm)
```

2. Test for assumptions


```{r}
#| layout-ncol: 2
#| echo: fenced

plot(mlinm)
```

3. ***Bonus 1***: Model Comparison

we can compare multiple model with `anova(_)` function

-   for example we have null model

```{r}
#| echo: fenced

slinnulm <- lm(BMI_Diff ~ 1, asthmads)
```

-   we want to compare our simple linear regression model with the null model

```{r}
#| echo: fenced

anova(slinnulm, slinm)
```

-   we can also compare multiple models

```{r}
#| echo: fenced

anova(slinnulm, slinm, mlinm)
```

-   we can see that our multiple linear regression model is not significantly different with simple linear regression model. thus we can choose the simpler model i.e., the simple linear regression model.

4. ***Bonus 2***: Variable Selection

:::callout-important
variable selection should start with plausible relationship i.e., back up with prior knowledge. this should start even at study design, in which unlikely relationship should not be included as variable collected in the study. this is to avoid spurious significant association

further read: https://doi.org/10.1002%2Fbimj.201700067
:::

While variable selection should be made purposely, i.e., the PI specifically select variable to be included in final model, variable selection by statistically software is still available to conduct.

-   create null model

```{r}
#| echo: fenced

mlinnulm <- lm(BMI_Diff ~ 1, asthmads)
```

-   create full model

```{r}
#| echo: fenced

mlinfulm <- lm(BMI_Diff ~ Gender + Age + WorkStatus + PA_HW + BMI_PreCat, 
               asthmads)
```

-   conduct forward selection

```{r}
#| echo: fenced

step(mlinfulm, 
     scope = list(lower = mlinnulm, upper = mlinfulm), 
     direction = "forward", 
     trace = 0)
```

-   conduct backward selection

```{r}
#| echo: fenced

step(mlinfulm, 
     scope = list(lower = mlinnulm, upper = mlinfulm), 
     direction = "backward", 
     trace = 0)
```

-   conduct both selection

```{r}
#| echo: fenced

step(mlinfulm, 
     scope = list(lower = mlinnulm, upper = mlinfulm), 
     direction = "both", 
     trace = 0)
```

5. ***Bonus 3***: Nice table with gtsummary
    -   final model table based on variable selection, direction "both"

```{r}
#| echo: fenced

lm(formula = BMI_Diff ~ Gender + WorkStatus + PA_HW, 
   data = asthmads) %>% 
  tbl_regression() %>% 
  bold_p()
```

6. ***Bonus 4***: Another package that also create nice table, sjPlot

```{r}
#| echo: fenced

library(sjPlot)

lm(formula = BMI_Diff ~ Gender + WorkStatus + PA_HW, 
   data = asthmads) %>% 
  tab_model()
```


## Logistic Regression

Similar to linear regression, logistic regression models the relationship between a dependent variable and one or more independent variables. However, while linear regression predicts continuous outcomes, binary logistic regression is used when the outcome is binary in nature, with two possible outcomes (0/1, yes/no, true/false). Logistic regression estimates the **probabilities** of the binary outcome

the formula is

$$\log\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1X$$

-   $P$ is the probability of outcome
-   $\frac{P}{1-P}$ is the odd of event occuring
-   $\log\left(\frac{P}{1-P}\right)$ - the log odds or logit
-   $X$ is the predictor
-   $\beta_0$ is the y-intercept
-   $\beta_1X$ is the slope of the line, representing the change of probability in y for one-unit change in  $X$

The assumptions for logistic regression are:  

-   each observation were **indepedent**.
-   the outcome variable is **binary categorical** data
-   The log odds of the outcome should have a **linear** relationship with the independent variables
-   abscence (or minimal) **multicollinearity**
-   **large sample size**. general rule of thumn, at least 10 cases with the least frequent outcome for each independent variable in the model
-   no extreme **outlier**
-   model should adequately fit the model, i.e., test for **goodness-of-fit**

:::callout-tip
note the similarities and differences between linear regression and logistic regression. 
:::


Example: Finding factors associated with BMI changes category

### Simple Logistic Regression

1. We will use the same dataset as previous, i.e., the `asthmads`
    -   we need to do some data wrangling to categorised the BMI changes.
    -   assumed, if the BMI changes (reduced) by 2.5 kg/m^2^, the changes is effective

```{r}
#| echo: fenced

asthmads <- asthmads %>% 
  mutate(BMI_DiffCat = cut(BMI_Diff, 
                           breaks = c(0, -2, -6), 
                           labels = c("Effective", "Not Effective")), 
         BMI_DiffCat = fct_relevel(BMI_DiffCat, "Not Effective"))
```

2. Conduct the simple logistic regression

```{r}
#| echo: fenced

slogm <- glm(BMI_DiffCat ~ PA_HW, family = binomial, asthmads)

summary(slogm)
```

3. Exponentiate the estimate and confident interval

```{r}
#| echo: fenced

exp(coef(slogm))

exp(confint(slogm))
```

:::callout-note
exponentiating a coefficient gives you the odds ratio associated with a one-unit increase in the corresponding predictor variable, assuming other variables are held constant
:::

4. Test assumption
    -   similar to linear regression, simplest way for assumption testing is use `plot(_)` function

```{r}
#| layout-ncol: 2
#| echo: fenced

plot(slogm)
```

5. Check model fitness

-   in R, the model fitness measured with AIC, which is from the `summary(_)` function

```{r}
#| echo: fenced

summary(slogm)
```

-   we can also conduct hosmer-lemeshow test.
-   the outcome however need to change to binary numeric 0/1 first.

```{r}
#| echo: fenced

asthmads <- asthmads %>% 
  mutate(BMI_DiffCatN = case_when(BMI_DiffCat == "Not Effective" ~ 0, 
                                  BMI_DiffCat == "Effective" ~ 1))

library(ResourceSelection)

hoslem.test(asthmads$BMI_DiffCatN, 
            fitted(slogm), g = 5)
```

6. Check model performance

-   model performance with accuracy, sensitivity and specificity
    -   need to calculate the predicted outcome, using `broom::` package

```{r}
#| echo: fenced

library(caret)
library(broom)

augment(slogm, type.predict = "response") %>% 
  mutate(predcat = cut(.fitted, 
                       breaks = c(0, .5, 1), 
                       labels = c("Not Effective", "Effective")), 
         predcat = fct_relevel(predcat, "Not Effective"))  %>% 
  with(., table(BMI_DiffCat, predcat)) %>% 
  confusionMatrix(., positive = "Effective")
```

- model performance with AUROC

```{r}
#| echo: fenced

library(pROC)

augment(slogm, type.predict = "response") %>% 
  mutate(predcat = cut(.fitted, 
                       breaks = c(0, .5, 1), 
                       labels = c("Not Effective", "Effective")), 
         predcat = fct_relevel(predcat, "Not Effective")) %>% 
  roc(BMI_DiffCat, .fitted, 
      levels = c("Not Effective", "Effective"), 
      percent = T, 
      ci = T, 
      plot = T, 
      smooth = F)
```

- of course if you want to customise the plot with ggplot, you can extract the value

```{r}
augment(slogm, type.predict = "response") %>% 
  mutate(predcat = cut(.fitted, 
                       breaks = c(0, .5, 1), 
                       labels = c("Not Effective", "Effective")), 
         predcat = fct_relevel(predcat, "Not Effective")) %>% 
  roc(BMI_DiffCat, .fitted, 
      levels = c("Not Effective", "Effective")) %>% 
  coords()
  
augment(slogm, type.predict = "response") %>% 
  mutate(predcat = cut(.fitted, 
                       breaks = c(0, .5, 1), 
                       labels = c("Not Effective", "Effective")), 
         predcat = fct_relevel(predcat, "Not Effective")) %>% 
  roc(BMI_DiffCat, .fitted, 
      levels = c("Not Effective", "Effective")) %>% 
  coords() %>% 
  arrange(sensitivity, specificity) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity)) + 
  geom_line(group = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = 2) + 
  theme_bw() +
  theme(aspect.ratio = 1)


```

